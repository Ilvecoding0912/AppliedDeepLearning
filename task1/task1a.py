import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import time
from task import *

def loss_fc(y, t, w, lam):
    '''
    Return the Ridge loss of the model

    Args:
    y (Torch.tensor): predicted value generated by model
    t (Torch.tensor): target data
    w (Torch.tensor): weight vector
    lam (float): ratio of norm of w 
    '''
    return torch.mean(torch.square((y-t))) + lam * torch.norm(w)

def sgd_M(x, t, M, lr=3e-13, batch_size=10, lam=100):
    '''
    Implement a stochastic minibatch gradient descent algorithm for fitting the polynomial functions
    This function also returns the optimum weight vector w


    Args:
    X (Torch.tensor): Nx1 vector
    t (Torch.tensor): Nx1 target values
    M (int): polynomial degree
    lr (float): learning rate
    batch_size (int): size of minibatch

    Return:
    w (torch.tensor): optimum weight vector
    '''
    
    num_epochs = 200
    num_batches = int(np.ceil(len(x) / batch_size))

    # Initialize w
    w = torch.randn(M+1, requires_grad=True)
    optimizer = optim.SGD([w], lr=lr)

    print('---------------------------SGD training process--------------------------')
    print(f'M value: {M}')
    for epoch in range(num_epochs):
        epoch_loss = 0
        indices = torch.randperm(len(x))

        for batch in range(num_batches):
            # Get mini-batch indices
            start_idx = batch * batch_size
            end_idx = min(start_idx + batch_size, len(x))
            batch_idx = indices[start_idx:end_idx]

            x_batch = x[batch_idx]
            t_batch = t[batch_idx]

            y_pred = polynomial_fun(w, x_batch)
            loss = loss_fc(y_pred, t_batch, w, lam)

            epoch_loss += loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        epoch_loss = epoch_loss / num_batches

        # Print the loss every 10 epochs
        if (epoch+1) % 10 == 0:
            print(f"Epoch: {epoch+1}, Loss: {epoch_loss.item():.4f}")
            
    return w

if __name__ == '__main__':
    w, x_train, x_test, y_train, y_test, x, y = generate_data()

    # Calculate observed training data by adding guassian noise
    mean, mean_test = torch.zeros(len(y_train)), torch.zeros(len(y_test))
    std, std_test = torch.ones(len(y_train)) * 0.2, torch.ones(len(y_test)) * 0.2
    t = y_train + torch.normal(mean=mean, std=std)
    t_test = y_test + torch.normal(mean=mean_test, std=std_test)

    # Find optimal M
    M_list = torch.arange(0, 6, 1)
    lr_list = [1e-7, 1e-10, 1e-13]
    min_loss, min_loss_M = torch.inf, torch.inf

    for M in M_list:
        for lr in lr_list:
            # First search for which learning rate is the best for the M value
            w = sgd_M(x_train, t, M=M, lr=lr, batch_size=10)
            y_pred_test = polynomial_fun(w, x_test)
            loss = loss_fc(y_pred_test, t_test, w, lam=100)

            if loss < min_loss:
                min_loss = loss
                best_lr = lr

        # Then search for the best M value
        w = sgd_M(x_train, t, M=M, lr=best_lr, batch_size=10)
        y_pred_test = polynomial_fun(w, x_test)
        loss = loss_fc(y_pred_test, t_test, w, lam=100)

        if min_loss < min_loss_M:
            min_loss_M = min_loss
            best_M = M
            best_w = w
    
    y_pred = polynomial_fun(best_w, x)
    
    print('\n')
    print(f'Optimised M value: {best_M}     Best learning rate: {best_lr}')

    print('------------Between predicted value and ground truth------------')
    print(f'| Mean difference: {torch.mean(y_pred - y)} \t| Standard deviation: {torch.std(y_pred - y)}')
    print('\n')